{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mVPdHF2v8yD",
        "outputId": "d51730e9-9a63-4434-e1e4-6e0b29f88e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mul.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile mul.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void matmul(int* A, int* B, int* C, int N) {\n",
        "    int Row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int Col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (Row < N && Col < N) {\n",
        "        int Pvalue = 0;\n",
        "        for (int k = 0; k < N; k++) {\n",
        "            Pvalue += A[Row * N + k] * B[k * N + Col];\n",
        "        }\n",
        "        C[Row * N + Col] = Pvalue;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 512;\n",
        "    int size = N * N * sizeof(int);\n",
        "    int *A, *B, *C;\n",
        "    int *dev_A, *dev_B, *dev_C;\n",
        "\n",
        "    // Allocate pinned memory on host for better performance\n",
        "    cudaMallocHost((void**)&A, size);\n",
        "    cudaMallocHost((void**)&B, size);\n",
        "    cudaMallocHost((void**)&C, size);\n",
        "\n",
        "    // Allocate memory on device\n",
        "    cudaMalloc((void**)&dev_A, size);\n",
        "    cudaMalloc((void**)&dev_B, size);\n",
        "    cudaMalloc((void**)&dev_C, size);\n",
        "\n",
        "    // Initialize matrices A and B\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            A[i * N + j] = i * N + j;\n",
        "            B[i * N + j] = j * N + i;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Copy matrices to device\n",
        "    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define block and grid size\n",
        "    dim3 dimBlock(16, 16);\n",
        "    dim3 dimGrid((N + dimBlock.x - 1) / dimBlock.x, (N + dimBlock.y - 1) / dimBlock.y);\n",
        "\n",
        "    // Launch kernel\n",
        "    matmul<<<dimGrid, dimBlock>>>(dev_A, dev_B, dev_C, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print a portion of the result matrix\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        for (int j = 0; j < 10; j++) {\n",
        "            std::cout << C[i * N + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(dev_A);\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "    cudaFreeHost(A);\n",
        "    cudaFreeHost(B);\n",
        "    cudaFreeHost(C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/cuda           # Removes any previous CUDA installations (only needed in certain environments).\n",
        "!ln -s  /usr/local/cuda-12.5 /usr/local/cuda        # Links to CUDA 12.2.\n",
        "!nvcc -arch=sm_75 mul.cu -o mul         #"
      ],
      "metadata": {
        "id": "Bq9uZu3WwPrI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./mul"
      ],
      "metadata": {
        "id": "PsnznQBfwT54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d1a821-5932-4b84-e012-7d1e22469acb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44608256 111586048 178563840 245541632 312519424 379497216 446475008 513452800 580430592 647408384 \n",
            "111586048 312781568 513977088 715172608 916368128 1117563648 1318759168 1519954688 1721150208 1922345728 \n",
            "178563840 513977088 849390336 1184803584 1520216832 1855630080 -2103923968 -1768510720 -1433097472 -1097684224 \n",
            "245541632 715172608 1184803584 1654434560 2124065536 -1701270784 -1231639808 -762008832 -292377856 177253120 \n",
            "312519424 916368128 1520216832 2124065536 -1567053056 -963204352 -359355648 244493056 848341760 1452190464 \n",
            "379497216 1117563648 1855630080 -1701270784 -963204352 -225137920 512928512 1250994944 1989061376 -1567839488 \n",
            "446475008 1318759168 -2103923968 -1231639808 -359355648 512928512 1385212672 -2037470464 -1165186304 -292902144 \n",
            "513452800 1519954688 -1768510720 -762008832 244493056 1250994944 -2037470464 -1030968576 -24466688 982035200 \n",
            "580430592 1721150208 -1433097472 -292377856 848341760 1989061376 -1165186304 -24466688 1116252928 -2037994752 \n",
            "647408384 1922345728 -1097684224 177253120 1452190464 -1567839488 -292902144 982035200 -2037994752 -763057408 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Nd4JG3dwf5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bra6_zR-wfv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>  // Provides necessary functions and macros to work with CUDA.\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void addVectors(int* A, int* B, int* C, int n) //__global__: Specifies that this function is a CUDA kernel, meaning it runs on the GPU and is called from the CPU.\n",
        "{\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x; // blockIdx.x: The index of the block within the grid. blockDim.x: The number of threads in each block. threadIdx.x: The index of the thread within the block.\n",
        "    if (i < n) // Ensures that threads do not access memory beyond the allocated array.\n",
        "    {\n",
        "        C[i] = A[i] + B[i]; // Adds corresponding elements from vectors A and B, storing the result in C.\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int n = 1000000; // The size of the vectors (one million elements).\n",
        "    int* A, * B, * C; // Pointers for the host (CPU) memory.\n",
        "    int size = n * sizeof(int); // The memory size required for each vector in bytes.\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    cudaMallocHost(&A, size); //  Allocates pinned (page-locked) memory on the host. This improves the speed of memory transfer between host and device.\n",
        "    cudaMallocHost(&B, size);\n",
        "    cudaMallocHost(&C, size);\n",
        "\n",
        "    // Initialize the vectors\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        A[i] = i; // Fills A with values [0, 1, 2, ..., n-1].\n",
        "        B[i] = i * 2;  // Fills B with values [0, 2, 4, ..., 2*(n-1)].\n",
        "    }\n",
        "    // Allocate memory on the device\n",
        "    int* dev_A, * dev_B, * dev_C;\n",
        "    cudaMalloc(&dev_A, size); // cudaMalloc(&dev_A, size): Allocates size bytes of GPU memory for A.\n",
        "    cudaMalloc(&dev_B, size); // cudaMalloc(&dev_B, size): Allocates size bytes of GPU memory for B.\n",
        "    cudaMalloc(&dev_C, size); // cudaMalloc(&dev_C, size): Allocates size bytes of GPU memory for C.\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice); // cudaMemcpy(dest, src, size, cudaMemcpyHostToDevice): Copies data from host memory to device memory.\n",
        "    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the kernel\n",
        "    int blockSize = 256; // Defines 256 threads per block.\n",
        "    int numBlocks = (n + blockSize - 1) / blockSize; // Ensures that all elements are covered\n",
        "    addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n); // This launches the kernel with numBlocks blocks and blockSize threads per block. Each thread computes C[i] = A[i] + B[i] in parallel.\n",
        "\n",
        "    // Copy data from device to host\n",
        "    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost); // Copies the computed results from device memory (dev_C) to host memory (C).\n",
        "\n",
        "    // Print the results\n",
        "    for (int i = 0; i < 10; i++)\n",
        "    {\n",
        "        cout << C[i] << \" \"; // Prints the first 10 elements of C to verify the computation.\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(dev_A); // releases memory on the GPU.\n",
        "    cudaFree(dev_B);\n",
        "    cudaFree(dev_C);\n",
        "    cudaFreeHost(A); //  releases pinned memory on the CPU.\n",
        "    cudaFreeHost(B);\n",
        "    cudaFreeHost(C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eONq9Svlwgdy",
        "outputId": "71e804b2-506b-4120-8be8-c087fdc3b36b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing add.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/cuda           # Removes any previous CUDA installations (only needed in certain environments).\n",
        "!ln -s  /usr/local/cuda-12.5 /usr/local/cuda        # Links to CUDA 12.2.\n",
        "!nvcc -arch=sm_75 add.cu -o add         # Compiles the CUDA program (nvcc is the CUDA compiler)."
      ],
      "metadata": {
        "id": "jT1jKVUhwkrj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./add // Each element of C is the sum of the corresponding elements of A and B:C[0] = 0 + 0 = 0    C[1] = 1 + 2 = 3   C[2] = 2 + 4 = 6    C[3] = 3 + 6 = 9    C[4] = 4 + 8 = 12    ..."
      ],
      "metadata": {
        "id": "JtVg7wQ0wpVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99325f63-c790-46f9-f077-669b38a18797"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3 6 9 12 15 18 21 24 27 \n"
          ]
        }
      ]
    }
  ]
}