{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_MznfcKK0GO","executionInfo":{"status":"ok","timestamp":1742890198871,"user_tz":-330,"elapsed":32,"user":{"displayName":"Shital Wadaganve","userId":"01380132478388248213"}},"outputId":"e61ea26d-e03b-465c-f043-7c1a0496f419"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting add.cu\n"]}],"source":["%%writefile add.cu\n","\n","#include <iostream>\n","#include <cuda_runtime.h>  // Provides necessary functions and macros to work with CUDA.\n","\n","using namespace std;\n","\n","__global__ void addVectors(int* A, int* B, int* C, int n) //__global__: Specifies that this function is a CUDA kernel, meaning it runs on the GPU and is called from the CPU.\n","{\n","    int i = blockIdx.x * blockDim.x + threadIdx.x; // blockIdx.x: The index of the block within the grid. blockDim.x: The number of threads in each block. threadIdx.x: The index of the thread within the block.\n","    if (i < n) // Ensures that threads do not access memory beyond the allocated array.\n","    {\n","        C[i] = A[i] + B[i]; // Adds corresponding elements from vectors A and B, storing the result in C.\n","    }\n","}\n","\n","int main()\n","{\n","    int n = 1000000; // The size of the vectors (one million elements).\n","    int* A, * B, * C; // Pointers for the host (CPU) memory.\n","    int size = n * sizeof(int); // The memory size required for each vector in bytes.\n","\n","    // Allocate memory on the host\n","    cudaMallocHost(&A, size); //  Allocates pinned (page-locked) memory on the host. This improves the speed of memory transfer between host and device.\n","    cudaMallocHost(&B, size);\n","    cudaMallocHost(&C, size);\n","\n","    // Initialize the vectors\n","    for (int i = 0; i < n; i++)\n","    {\n","        A[i] = i; // Fills A with values [0, 1, 2, ..., n-1].\n","        B[i] = i * 2;  // Fills B with values [0, 2, 4, ..., 2*(n-1)].\n","    }\n","    // Allocate memory on the device\n","    int* dev_A, * dev_B, * dev_C;\n","    cudaMalloc(&dev_A, size); // cudaMalloc(&dev_A, size): Allocates size bytes of GPU memory for A.\n","    cudaMalloc(&dev_B, size); // cudaMalloc(&dev_B, size): Allocates size bytes of GPU memory for B.\n","    cudaMalloc(&dev_C, size); // cudaMalloc(&dev_C, size): Allocates size bytes of GPU memory for C.\n","\n","    // Copy data from host to device\n","    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice); // cudaMemcpy(dest, src, size, cudaMemcpyHostToDevice): Copies data from host memory to device memory.\n","    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n","\n","    // Launch the kernel\n","    int blockSize = 256; // Defines 256 threads per block.\n","    int numBlocks = (n + blockSize - 1) / blockSize; // Ensures that all elements are covered\n","    addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n); // This launches the kernel with numBlocks blocks and blockSize threads per block. Each thread computes C[i] = A[i] + B[i] in parallel.\n","\n","    // Copy data from device to host\n","    cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost); // Copies the computed results from device memory (dev_C) to host memory (C).\n","\n","    // Print the results\n","    for (int i = 0; i < 10; i++)\n","    {\n","        cout << C[i] << \" \"; // Prints the first 10 elements of C to verify the computation.\n","    }\n","    cout << endl;\n","\n","    // Free memory\n","    cudaFree(dev_A); // releases memory on the GPU.\n","    cudaFree(dev_B);\n","    cudaFree(dev_C);\n","    cudaFreeHost(A); //  releases pinned memory on the CPU.\n","    cudaFreeHost(B);\n","    cudaFreeHost(C);\n","\n","    return 0;\n","}\n"]},{"cell_type":"code","source":["!rm -rf /usr/local/cuda           # Removes any previous CUDA installations (only needed in certain environments).\n","!ln -s  /usr/local/cuda-12.5 /usr/local/cuda        # Links to CUDA 12.2.\n","!nvcc -arch=sm_75 add.cu -o add         # Compiles the CUDA program (nvcc is the CUDA compiler)."],"metadata":{"id":"4pz10Ie6Lo0n","executionInfo":{"status":"ok","timestamp":1742890202071,"user_tz":-330,"elapsed":2619,"user":{"displayName":"Shital Wadaganve","userId":"01380132478388248213"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!./add // Each element of C is the sum of the corresponding elements of A and B:C[0] = 0 + 0 = 0    C[1] = 1 + 2 = 3   C[2] = 2 + 4 = 6    C[3] = 3 + 6 = 9    C[4] = 4 + 8 = 12    ..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lXexO0J1MHov","executionInfo":{"status":"ok","timestamp":1742890202421,"user_tz":-330,"elapsed":326,"user":{"displayName":"Shital Wadaganve","userId":"01380132478388248213"}},"outputId":"87cb291b-cf26-4ef8-ef21-70a5e696db1d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0 3 6 9 12 15 18 21 24 27 \n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mjoWbDz8HSAM"},"execution_count":null,"outputs":[]}]}